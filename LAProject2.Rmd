---
title: "Team Project 2" 
author: "Abizer and Luis"
date: "11/19/2022"
output: 
  github_document:
    pandoc_args: --webtex
---

In this project, we used the training data set to create a model that predicts the county- level winner of the 2016 Presidential Election. The winner of each county is classified as Democrat (Clinton) and Republican (Rep)


```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse=TRUE, prompt=TRUE,comment=NULL,message=FALSE, echo = FALSE, include=TRUE, fig.width = 9, fig.height = 4) #This code chunk ensures that there are no commands in output file.
```

```{r packageCheck, include=FALSE}
mypacks <- c("ggplot2","dplyr","readr","tidyr", "ROCR", "boot","class","randomForest","e1071", "stringr","partykit","rpart","plyr")  # what packages are needed?
packs <- installed.packages()   # find installed package list
install.me <- mypacks[!(mypacks %in% packs[,"Package"])]  #what needs to be installed?
if (length(install.me) >= 1) install.packages(install.me, repos = "http://cran.us.r-project.org")   # install (if needed)
lapply(mypacks, library, character.only=TRUE)  # load all packages
```

```{r}
key <- read.csv("https://raw.githubusercontent.com/mgelman/data/master/county_facts_dictionary.csv") #Read the key file
```


```{r}
train <- read_csv("https://raw.githubusercontent.com/mgelman/data/master/train.csv")
test <- read_csv("https://raw.githubusercontent.com/mgelman/data/master/test_No_Y.csv")
#Create the train and test data frames
```

We were concerned that there would be NA values in the “winner16” variable. To remove the NAs, we created a new variable called “winner” that omits the NA values. “Winner” variable is classified as Democrat if the actual winner is “Dem” and Republican if the actual winner is “Rep”. For example, some of the winner variable values are shown below

```{r}
train_winner <- train %>% mutate(winner = recode_factor(winner16, Dem = "Democrat", Rep = "Republican")) %>% na.omit()
head(train_winner$winner)
```

The fun part was deciding the variables we wanted to use in order to create our model. We ran a regression analysis with the 51 predictor variables and deciphered that 17 of them were significant. However, we wanted to avoid overfitting hence we ran a regression analysis with these 17 variables against the winner variable and chose only those variables that were significant. We were also concerned that some of these variables might be correlated to each other which might be affecting their significance level. To examine this, we ran a individual regression analysis for each variable against the winner variable and chose the ones that were still significant. After doing this procedure, we concluded that retail sales, income/capita, median house value, percentage of population living in multi unit housing, languages other English spoken at home, percentage of population who have completed undergraduate degree or higher, percentage of population living in same housing for multiple years and percentage change in population, Persons/Household, Land Area, Non-Employer Establishments, Hispanic Owned Firms, Accommodation/Food Sales and Building Permits are significant.

```{r}
library(plyr)

train_rename <- train_winner %>% plyr::rename(c("RTN130207"="Retail_Sales_07","INC910213"="Income_per_capita","HSG096213"="Percent_multi_unit_housing","POP715213"="Percent_living_in_same_house_multiple_years","HSG495213"="Median_house_value","POP815213"="Spoken_non_english_lang","EDU635213"="Percent_Highschool_grad","EDU685213"="Percent_Undergrad","AGE295214"="percent_under_18","AGE775214"="percent_over_65","PST120214"="percent_change_in_pop","HSD310213"="Persons_per_Household","LND110210"="Land_Area","NES010213"= "Nonemployer_establishments","SBO415207"="Hispanic_firms","AFN120207"="Accomodation_Food_Sales", "BPS030214"="Building_Permits"))
# Here we renamed the variables that we were going to use in the train dataset and saved the data frame as train1
test_rename <- test %>% plyr::rename(c("RTN130207"="Retail_Sales_07","INC910213"="Income_per_capita","HSG096213"="Percent_multi_unit_housing","HSG495213"="Median_house_value","POP815213"="Spoken_non_english_lang","EDU635213"="Percent_Highschool_grad","POP715213"="Percent_living_in_same_house_multiple_years","EDU685213"="Percent_Undergrad","AGE295214"="percent_under_18","AGE775214"="percent_over_65","PST120214"="percent_change_in_pop","HSD310213"="Persons_per_Household","LND110210"="Land_Area","NES010213"= "Nonemployer_establishments","SBO415207"="Hispanic_firms", "AFN120207"="Accomodation_Food_Sales","BPS030214"="Building_Permits")) 
# here we renamed the variables we were going to use in the test dataset and saved the object as test1

train_final <- train_rename %>% select(Retail_Sales_07, Income_per_capita, Median_house_value,Percent_multi_unit_housing, Spoken_non_english_lang, Percent_Undergrad,  Percent_living_in_same_house_multiple_years, percent_change_in_pop, Persons_per_Household,Land_Area, Nonemployer_establishments, Hispanic_firms, Accomodation_Food_Sales, Building_Permits, winner)
# here we create a new dataframe called Train1 that has te variables that we want

test_final <- test_rename %>% select(Retail_Sales_07, Income_per_capita, Median_house_value, Spoken_non_english_lang, Percent_multi_unit_housing,Percent_living_in_same_house_multiple_years, Percent_Undergrad, percent_change_in_pop, Persons_per_Household,Land_Area,Nonemployer_establishments, Hispanic_firms, Accomodation_Food_Sales, Building_Permits)
# here we create a new dataframe called Test1 that has the variables that we want

xvars <- str_c(names(train_final)[1:14], collapse="+")
myform <- as.formula(str_c("winner ~ ", xvars))
#x-variables are the names of the first 8 columns in the Train1 dataset

winner.glm <- glm(myform, data = train_final, family = binomial)
# we created a model based on the Train1 dataset

summary(winner.glm)
```

We then fit the model "winner.glm" into the train and test data to create probabilities. We then set a threshold of 0.05. For example, if the model determines the probability to be 0.1, prediction will be Republican or else Democrat.  Then, we created a double density curve

```{r}
train_final <- train_final %>% mutate(probs = predict(winner.glm, type = "response"),prediction = ifelse(probs >= 0.05,"Republican","Democrat"))
# here we fitted the model winner.glm to the training data and included the probabilities and predictions

test_final <- test_final %>% mutate(probs = predict(winner.glm, newdata = test_final, type = "response"),prediction = ifelse(probs >= 0.05,"Republican","Democrat"))
# here we fitted the model winner.glm to the testing data and included the probabilities and predictions

ggplot(train_final,aes(x = probs, color = winner)) + geom_density(size = 1.5) + ggtitle("Forecasted Winner Probabilities ")

# we created a double density curve
```

The double density curve indicates that most Republicans have an estimated probability of winning around 90%. Probability of Democrats winning is not defined so well but range lower than a Republican

Here, we created a ROC curve for the training data
```{r}
preds_obj1 <- prediction(train_final$probs, train_final$winner, label.ordering=c("Democrat","Republican"))
perf_obj1 <- performance(preds_obj1, "tpr","fpr")
perf_df1 <- data_frame(fpr=unlist(perf_obj1@x.values),
                       tpr= unlist(perf_obj1@y.values),
                       threshold=unlist(perf_obj1@alpha.values), 
                       model="train")

ggplot(perf_df1, aes(x=fpr, y=tpr, color=model)) +  geom_line(size=1.5) + 
  labs(x="false positive rate", y="true positive rate", title="ROC curve for logistic regression") + 
  geom_abline(slope=1,intercept=0, linetype=3) 

# here we created a ROC curve
```

The ROC curve shows the true positive and false positive rate of our model. ROC curve suggests that our model is very good because it is close to a right angled triangle and has a higher sensitivity than a random classifier.

We then calculated the accuracy, precision and recall and the error of the model using a 5-fold cross validation and a 0.05 threshold

```{r}

train_final %>% summarize(accuracy = mean(winner == prediction), precision = sum(winner == "Republican" & prediction == "Republican")/sum(prediction == "Republican"), recall = sum(winner == "Republican" & prediction == "Republican")/sum(winner == "Republican"))
# here we calculated the accuracy, precision and recall by comparing our predictions to the actual winners

set.seed(5)
cost <- function(y, pi) 1-mean(y==(pi>0.05))
train_error <- cv.glm(data = train_final, winner.glm,cost,5)
train_error$delta[1]
# here we used a 5-fold cross validation to calculate the error for logistic regression model
```

The accuracy for the logistic regression model is 0.883 The error is 0.119

Random Forest

We then created a random forest model called winner_rforest with 350 trees that randomly select out of 5 variables at each split. To choose the number of bootstrapped trees and the number of predictors to randomly select, we tried numbers around 350 and 5 respectively. We noticed that the accuracy goes down if we input more/less predictors or more/less trees hence we decided to stick with 350 trees and 5 predictors to randomly select. Then we fitted the model to the training data, and calculated the accuracy, precision, recall and error for the training data predictions. 


```{r}
set.seed(5)

winner_rforest <- randomForest(myform,data = train_final, ntree = 350, mtry = 5)
# we created a random forest model that has 400 trees and randomly selects out of 4 variables at each split
winner_rforest

train_rforest <- train_final %>% mutate(probs = predict(winner_rforest, type = "prob")[,2],prediction = predict(winner_rforest, type = "response"))

# we created a new data frame called train3 in which we fitted the random forest model that we created to the training dataset and computed the predictions 

test_reforest <- test_final %>% mutate(prediction = predict(winner_rforest,type = "response",newdata = test_final))

# we created a new data frame called test3 in which we fitted the random forest model that we created to the testing dataset and computed the predictions 

train_rforest %>% summarize(accuracy = mean(winner == prediction), precision = sum(winner == "Republican" & prediction == "Republican")/sum(prediction == "Republican"), recall = sum(winner == "Republican" & prediction == "Republican")/sum(winner == "Republican"), error = 1 - accuracy)

# here we computed the accuracy, precision, recall and error of our random forest model by comparing the predictions to the actual winners

```

Our accuracy was 0.91 and error was 0.09. 

After running the two models, it was clear that the random forest model gave us the highest accuracy of 0.91 and lowest error of 0.09 so we went ahead with that model for our predictions.


```{r}
test_No_Y <- test_rename %>% mutate(pred_winner = predict(winner_rforest, type = "response",newdata = test_rename))
write_csv(test_No_Y, "test_No_Y_MamnoonGomez.csv")
```
