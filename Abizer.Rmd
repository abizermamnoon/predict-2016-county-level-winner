---
title: "Team Project 2" 
author: "Abizer and Luis"
date: "Day 17"
output: 
  github_document:
    pandoc_args: --webtex
---
This code chunk ensures that there are no commands in output file

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse=TRUE, prompt=TRUE,comment=NULL,message=FALSE, echo = TRUE, include=TRUE, fig.width = 9, fig.height = 4)
```

Download the packages

```{r packageCheck, include=FALSE}
mypacks <- c("ggplot2","dplyr","readr","tidyr", "ROCR", "boot","class","randomForest","e1071", "stringr","partykit","rpart","plyr")  # what packages are needed?
packs <- installed.packages()   # find installed package list
install.me <- mypacks[!(mypacks %in% packs[,"Package"])]  #what needs to be installed?
if (length(install.me) >= 1) install.packages(install.me, repos = "http://cran.us.r-project.org")   # install (if needed)
lapply(mypacks, library, character.only=TRUE)  # load all packages
```

Read the key file
```{r}
key <- read.csv("https://raw.githubusercontent.com/mgelman/data/master/county_facts_dictionary.csv")

```

Create the train and test dataframes
```{r}
train <- read_csv("https://raw.githubusercontent.com/mgelman/data/master/train.csv")
test <- read_csv("https://raw.githubusercontent.com/mgelman/data/master/test_No_Y.csv")
```

Define a winner variable, which is based on winner16 variable. Winner is set as Democrat if winner16 is Dem and Republican if winner16 is Rep

```{r}
train <- train %>% mutate(winner = recode_factor(winner16, Dem = "Democrat", Rep = "Republican"))
```

We chose the x-variables by determining which variables were most significant using a regression analysis

```{r}
library(plyr)
train1 <- train %>% plyr::rename(c("RTN130207"="Retail_Sales_07","INC910213"="Income_per_capita","HSG096213"="Percent_multi_unit_housing","HSG495213"="Median_house_value","POP815213"="Spoken_non_english_lang","EDU635213"="Percent_Highschool_grad","EDU685213"="Percent_Undergrad","POP715213"="Percent_living_in_same_house_multiple_years","AGE295214"="percent_under_18","AGE775214"="percent_over_65","PST120214"="percent_change_in_pop"))

test1 <- test %>% plyr::rename(c("RTN130207"="Retail_Sales_07","INC910213"="Income_per_capita","HSG096213"="Percent_multi_unit_housing","HSG495213"="Median_house_value","POP815213"="Spoken_non_english_lang","EDU635213"="Percent_Highschool_grad","EDU685213"="Percent_Undergrad","POP715213"="Percent_living_in_same_house_multiple_years","AGE295214"="percent_under_18","AGE775214"="percent_over_65","PST120214"="percent_change_in_pop"))
```

```{r}
Train1 <- train1 %>% select(Retail_Sales_07, Income_per_capita, Percent_multi_unit_housing, Median_house_value, Spoken_non_english_lang, Percent_Highschool_grad, Percent_Undergrad, Percent_living_in_same_house_multiple_years, percent_under_18, percent_over_65, percent_change_in_pop,winner)

xvars <- str_c(names(train1)[1:11], collapse="+")
myform <- as.formula(str_c("winner ~ ", xvars))

winner.glm <- glm(myform, data = Train1, family = binomial)

summary(winner.glm)
```


We then fit the model "winner.glm" into the train and test data to create probabilities. We then set a threshold of 0.05. For example, if the model determines the probability to be 0.1, prediction will be Republican or else Democrat. We then calculated the accuracy, precision and recall for the model. Then, we created a double density curve

```{r}
train <- Train1 %>% mutate(probs = predict(winner.glm, type = "response"),prediction = ifelse(probs >= 0.05,"Republican","Democrat"))

test <- test1 %>% mutate(probs = predict(winner.glm, newdata = test1, type = "response"),prediction = ifelse(probs >= 0.05,"Republican","Democrat"))

train %>% summarize(accuracy = mean(winner == prediction), precision = sum(winner == "Republican" & prediction == "Republican")/sum(prediction == "Republican"), recall = sum(winner == "Republican" & prediction == "Republican")/sum(winner == "Republican"))

ggplot(train,aes(x = probs, color = winner)) + geom_density(size = 1.5) + ggtitle("Forecasted Winner Probabilities ")
```

Here, we created a ROC curve for the training data
```{r}
preds_obj1 <- prediction(train$probs, train$winner, label.ordering=c("Democrat","Republican"))
perf_obj1 <- performance(preds_obj1, "tpr","fpr")
perf_df1 <- data_frame(fpr=unlist(perf_obj1@x.values),
                       tpr= unlist(perf_obj1@y.values),
                       threshold=unlist(perf_obj1@alpha.values), 
                       model="train")

ggplot(perf_df1, aes(x=fpr, y=tpr, color=model)) +  geom_line(size=1.5) + 
  labs(x="false positive rate", y="true positive rate", title="ROC curve for logistic regression") + 
  geom_abline(slope=1,intercept=0, linetype=3) 
```

I calculated the error of the model using a 5-fold cross validation and a 0.05 threshold
```{r}
set.seed(5)
cost <- function(y, pi) 1-mean(y==(pi>0.05))
train_error <- cv.glm(data = train, winner.glm,cost,5)
train_error$delta[1]
```
Random Forest

Created a random forest model with 300 trees that randomly select out of 5 variables at each split. Then we fitted the model to the training data and the testing data. We then calculated the accuracy, precision, recall and error for the training data predictions
```{r}
set.seed(5)

winner_rforest <- randomForest(myform,data = train, ntree = 300, mtry = 5)

train3 <- train %>% mutate(probs = predict(winner_rforest, type = "prob")[,2],prediction = predict(winner_rforest, type = "response"))

test3 <- test %>% mutate(prediction = predict(winner_rforest,type = "response",newdata = test))

train3 %>% summarize(accuracy = mean(winner == prediction), precision = sum(winner == "Republican" & prediction == "Republican")/sum(prediction == "Republican"), recall = sum(winner == "Republican" & prediction == "Republican")/sum(winner == "Republican"), error = 1-accuracy)
```

K-nn 

```{r}
set.seed(7)
n <- nrow(train1)

xvars <- str_c(names(train1)[1:11], collapse="+")

train_index <- sample(1:n, size=round(.8*n))

train2 <- train %>% slice(train_index) %>% select( Retail_Sales_07, Income_per_capita, Percent_multi_unit_housing, Median_house_value, Spoken_non_english_lang, Percent_Highschool_grad, Percent_Undergrad, Percent_living_in_same_house_multiple_years, percent_under_18, percent_over_65, percent_change_in_pop,winner)

test2 <- test %>% slice(-train_index) %>% select( Retail_Sales_07, Income_per_capita, Percent_multi_unit_housing, Median_house_value, Spoken_non_english_lang, Percent_Highschool_grad, Percent_Undergrad, Percent_living_in_same_house_multiple_years, percent_under_18, percent_over_65, percent_change_in_pop)


```
train_knn <- knn(train2[, -1], test2, train2$winner[train_index], k=10)

train4 <- data_frame( y=train2[-train_index],prediction = train_knn) %>% 
  summarize(accuracy = mean(prediction == y), 
                   precision = sum(prediction == "Default" & y == "Default")/sum(prediction == "Default"),
                   recall = sum(prediction == "Default" & y == "Default")/sum(y == "Default"),
                   ) 

train_knn <- knn(train2[, -1], test2, train2$winner[train_index], k=10)

train4 <- data_frame( y=train2[-train_index],prediction = train_knn) %>% 
  summarize(accuracy = mean(prediction == y), 
                   precision = sum(prediction == "Default" & y == "Default")/sum(prediction == "Default"),
                   recall = sum(prediction == "Default" & y == "Default")/sum(y == "Default"),
                   ) 

